{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from activation import *\n",
    "from loss import *\n",
    "from linear import * \n",
    "from encapsulage import *\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def onehot(x): \n",
    "    out = [0] * 10\n",
    "    out[x] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (100,784) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 80\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# # Perte\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# loss = BCELoss()\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Entraînement\u001b[39;00m\n\u001b[0;32m     79\u001b[0m optim \u001b[38;5;241m=\u001b[39m Optim(net, loss, gradient_step)\n\u001b[1;32m---> 80\u001b[0m lloss \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kimsa\\OneDrive\\Bureau\\M1\\S2\\ML\\DIY-Project\\notebook\\..\\encapsulage.py:197\u001b[0m, in \u001b[0;36mOptim.SGD\u001b[1;34m(self, data_x, data_y, batch_size, epoch)\u001b[0m\n\u001b[0;32m    195\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m liste_batch_x[j]\n\u001b[0;32m    196\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m liste_batch_y[j]\n\u001b[1;32m--> 197\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     loss_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    199\u001b[0m loss_batch \u001b[38;5;241m=\u001b[39m loss_batch \u001b[38;5;241m/\u001b[39m nb_batches\n",
      "File \u001b[1;32mc:\\Users\\kimsa\\OneDrive\\Bureau\\M1\\S2\\ML\\DIY-Project\\notebook\\..\\encapsulage.py:103\u001b[0m, in \u001b[0;36mOptim.step\u001b[1;34m(self, batch_x, batch_y)\u001b[0m\n\u001b[0;32m    101\u001b[0m liste_forwards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mforward(batch_x)\n\u001b[0;32m    102\u001b[0m batch_y_hat \u001b[38;5;241m=\u001b[39m liste_forwards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 103\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward(batch_y, batch_y_hat)\n\u001b[0;32m    105\u001b[0m list_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mbackward_delta(liste_forwards, delta)\n",
      "File \u001b[1;32mc:\\Users\\kimsa\\OneDrive\\Bureau\\M1\\S2\\ML\\DIY-Project\\notebook\\..\\loss.py:122\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, y, yhat)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, yhat):\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m        numpy.ndarray : coût ; taille = batch_size\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m (\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43myhat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-20\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m p\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m yhat \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-20\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,) (100,784) "
     ]
    }
   ],
   "source": [
    "# Charger les données MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalisation\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# One-hot encoding\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test[:100]\n",
    "y_test = y_test[:100]\n",
    "#y_train = np.array([onehot(x) for x in y_train])\n",
    "#y_test = np.array([onehot(x) for x in y_test])  \n",
    "\n",
    "\n",
    "\n",
    "# Paramètres\n",
    "# d = X_train.shape[1]\n",
    "# batchsize = 200\n",
    "# gradient_step = 1e-5\n",
    "\n",
    "\n",
    "# hidden_dim = 128\n",
    "# hidden_dim2 = 64\n",
    "# out_dim = 10\n",
    "# epoch = 1000\n",
    "\n",
    "\n",
    "\n",
    "# lin = Linear(d, hidden_dim)\n",
    "# lin2 = Linear(hidden_dim, hidden_dim2)\n",
    "# lin3 = Linear(hidden_dim2, hidden_dim)\n",
    "# lin4 = Linear(hidden_dim, out_dim)\n",
    "# lin3._parameters = lin2._parameters.T\n",
    "# lin4._parameters = lin._parameters.T\n",
    "\n",
    "# encodeur = [lin, TanH(), lin2, TanH()]\n",
    "# decodeur = [lin3, TanH(), lin4, Sigmoid()]\n",
    "\n",
    "# net = Sequentiel(encodeur + decodeur)\n",
    "\n",
    "\n",
    "# paramétrer le modèle\n",
    "d = X_train.shape[1]\n",
    "print(d)\n",
    "d_prime = 196\n",
    "d_prime2 = 100\n",
    "out = 784\n",
    "epoch = 10000\n",
    "gradient_step = 1e-5\n",
    "batchsize = 100\n",
    "lin = Linear(d, d_prime)\n",
    "lin2 = Linear(d_prime, d_prime2)\n",
    "lin3 = Linear(d_prime2, d_prime)\n",
    "lin4 = Linear(d_prime, out)\n",
    "lin3._parameters = lin2._parameters.T\n",
    "lin4._parameters = lin._parameters.T\n",
    "tan = TanH()\n",
    "sig = Sigmoid()\n",
    "loss = MSELoss()\n",
    "\n",
    "Encodeur = [lin,tan,lin2,tan]\n",
    "Decodeur = [lin3,tan,lin4,sig]\n",
    "\n",
    "net = Sequentiel(Encodeur + Decodeur)\n",
    "\n",
    "\n",
    "# # Perte\n",
    "# loss = BCELoss()\n",
    "\n",
    "# Entraînement\n",
    "optim = Optim(net, loss, gradient_step)\n",
    "lloss = optim.SGD(X_train, y_train, batchsize, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la perte\n",
    "plt.figure()\n",
    "plt.plot(lloss)\n",
    "plt.title(\"Perte au cours des époques\")\n",
    "plt.xlabel(\"Époques\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
